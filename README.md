NeuralLux-Bots
Lux AI Multi-Agent DQN
Deep Reinforcement Learning for Multi-Agent Strategy in Lux AI Season 3

Project Banner

ğŸ“Œ Table of Contents
Introduction
Features
Installation
Usage
Training Pipeline
Architecture
Results & Performance
Contributing
License
ğŸ¯ Introduction
This project implements Deep Q-Networks (DQN) for multi-agent reinforcement learning (MARL) in the Lux AI Challenge Season 3. The agents are trained to strategically navigate, collect resources, and compete effectively using a shared replay buffer and separate policy & target networks.

âœ¨ Features
âœ… Multi-Agent Deep Q-Learning for strategic decision-making. âœ… Experience Replay using a shared replay buffer. âœ… Target & Policy Networks to stabilize training. âœ… Epsilon-Greedy Exploration for improved performance. âœ… Adaptive Model Sampling to prevent overfitting to one strategy. âœ… Log-Based Reward Tracking for debugging.

âš™ï¸ Installation
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your_username/LuxAI-MultiAgentDQN.git
cd LuxAI-MultiAgentDQN
2ï¸âƒ£ Set Up Virtual Environment
python3 -m venv luxai_env
source luxai_env/bin/activate  # On Windows use `luxai_env\Scripts\activate`
3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
4ï¸âƒ£ Install Lux AI Environment
pip install luxai_s3
ğŸš€ Usage
Run the Environment with Your Bot
luxai-s3 path/to/your/bot.py path/to/opponent/bot.py --output replay.json
Train the Model
python train.py
Evaluate Performance
python evaluate.py --episodes 50
ğŸ§  Training Pipeline
1ï¸âƒ£ Observation Extraction: Converts raw environment state into agent-friendly representations. 2ï¸âƒ£ Action Selection: Uses the policy network (with epsilon-greedy exploration). 3ï¸âƒ£ Experience Storage: Saves (state, action, reward, next_state) tuples into a shared replay buffer. 4ï¸âƒ£ Batch Training:

Samples a mini-batch from replay buffer.
Updates policy network using TD-Target from target network.
Optimizes using MSE loss for Q-values. 5ï¸âƒ£ Target Network Update: Periodically syncs with policy network for stability.
ğŸ”§ Architecture
ğŸ“Œ Multi-Agent DQN Structure:

Agent.py -> Handles game interaction, reward computation
DQN.py -> Deep Q-Network, Policy & Target Network
ReplayBuffer.py -> Shared Experience Replay Buffer
Train.py -> Training Loop
Evaluate.py -> Performance Evaluation
ğŸ“Œ Neural Network Model:

Fully Connected Layers with ReLU activation
Q-Values Head: Predicts action-value estimates
X, Y Coordinate Heads: Predicts (x, y) positions for sapping actions
ğŸ“Š Results & Performance
ğŸ“Œ Training Progress

Episodes	Avg Reward	Win Rate (%)
100	-5.3	45.2
500	10.2	60.5
1000	18.7	75.8
ğŸ“Œ Sample Training Reward Curve Reward Curve (Replace path/to/reward_curve.png with your actual image!)

ğŸ’¡ Contributing
We welcome contributions! To contribute: 1ï¸âƒ£ Fork the repository. 2ï¸âƒ£ Create a feature branch. 3ï¸âƒ£ Commit changes with meaningful messages. 4ï¸âƒ£ Submit a pull request.

ğŸ“œ License
ğŸ“Œ This project is licensed under MIT License.
