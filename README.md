NeuralLux-Bots: Lux AI Multi-Agent DQN
Deep Reinforcement Learning for Multi-Agent Strategy in Lux AI Season 3

ğŸ“Œ Table of Contents
Introduction

Features

Installation

Usage

Training Pipeline

Architecture

Results & Performance

Contributing

License

ğŸ¯ Introduction
This project implements Deep Q-Networks (DQN) for multi-agent reinforcement learning (MARL) tailored for the Lux AI Challenge Season 3. Our agents learn to navigate the environment strategically, gather resources, and compete effectively by sharing experience through a replay buffer and maintaining separate policy and target networks for stable training.

âœ¨ Features
âœ… Multi-Agent Deep Q-Learning for complex strategic decision-making.

âœ… Experience Replay via a shared replay buffer to improve sample efficiency.

âœ… Target & Policy Networks to stabilize training and reduce divergence.

âœ… Epsilon-Greedy Exploration for balancing exploration and exploitation.

âœ… Adaptive Model Sampling to prevent overfitting on specific strategies.

âœ… Log-Based Reward Tracking for better debugging and performance monitoring.

âš™ï¸ Installation
1ï¸âƒ£ Clone the repository

bash
Copy
Edit
git clone https://github.com/your_username/LuxAI-MultiAgentDQN.git
cd LuxAI-MultiAgentDQN
2ï¸âƒ£ Set up a virtual environment

bash
Copy
Edit
python3 -m venv luxai_env
source luxai_env/bin/activate  # On Windows use: luxai_env\Scripts\activate
3ï¸âƒ£ Install dependencies

bash
Copy
Edit
pip install -r requirements.txt
4ï¸âƒ£ Install Lux AI environment

bash
Copy
Edit
pip install luxai_s3
ğŸš€ Usage
Run the environment with your bot:

bash
Copy
Edit
luxai-s3 path/to/your/bot.py path/to/opponent/bot.py --output replay.json
Train the model:

bash
Copy
Edit
python train.py
Evaluate performance:

bash
Copy
Edit
python evaluate.py --episodes 50
ğŸ§  Training Pipeline
1ï¸âƒ£ Observation Extraction: Transform raw environment state into agent-friendly features.
2ï¸âƒ£ Action Selection: Select actions using the policy network with epsilon-greedy exploration.
3ï¸âƒ£ Experience Storage: Store tuples (state, action, reward, next_state) into a shared replay buffer.
4ï¸âƒ£ Batch Training:

Sample mini-batches from the replay buffer.

Compute Temporal Difference (TD) targets using the target network.

Update policy network by minimizing mean squared error (MSE) loss of Q-values.
5ï¸âƒ£ Target Network Update: Periodically sync the target network weights with the policy network for training stability.

ğŸ”§ Architecture
Multi-Agent DQN Structure
Agent.py: Manages game interaction and reward computation.

DQN.py: Implements Deep Q-Networks, including policy and target networks.

ReplayBuffer.py: Shared experience replay buffer for multi-agent learning.

Train.py: Training loop and optimization logic.

Evaluate.py: Performance evaluation and testing.

Neural Network Model
Fully connected layers with ReLU activations.

Q-Values Head: Predicts action-value estimates for decision making.

X, Y Coordinate Heads: Predict (x, y) positions for sapping or positional actions.

ğŸ“Š Results & Performance
Episodes	Avg Reward	Win Rate (%)
100	-5.3	45.2
500	10.2	60.5
1000	18.7	75.8

Sample Training Reward Curve

Replace path/to/reward_curve.png with your actual reward curve image.

ğŸ’¡ Contributing
We welcome contributions! To contribute:
1ï¸âƒ£ Fork the repository
2ï¸âƒ£ Create a feature branch (git checkout -b feature-name)
3ï¸âƒ£ Commit your changes with meaningful messages (git commit -m "Add feature")
4ï¸âƒ£ Push to your branch (git push origin feature-name)
5ï¸âƒ£ Open a pull request

ğŸ“œ License
This project is licensed under the MIT License. See the LICENSE file for details.



ğŸ’¡ Contributing
We welcome contributions! To contribute: 1ï¸âƒ£ Fork the repository. 2ï¸âƒ£ Create a feature branch. 3ï¸âƒ£ Commit changes with meaningful messages. 4ï¸âƒ£ Submit a pull request.

ğŸ“œ License
ğŸ“Œ This project is licensed under MIT License.
